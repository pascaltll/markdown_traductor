# Содержание  

Учебник по машинному обучению  

![](images/d65809172d0d826cfd5df6d9ac9c02569ce45091ea65965c8e1edfed5dd273c4.jpg)  

# 11.1. Обучение с подкреплением  

Авторы  

![](images/32d40fbe70e1e4e4ab254a7a2aac15dfce151fb2bd65de2e54f43daff379842e.jpg)  

# Иванов Сергей  

До сих пор опыт, благодаря которому было возможно обучение в наших алгоритмах, был задан в виде обучающей выборки. Насколько такая модель обучения соотносится с тем, как учится, например, человек? Чтобы научиться кататься на велосипеде, печь тортики или играть в теннис, нам не нужны огромные датасеты с примерами того, что нужно делать в каждый момент; вместо этого мы способны обучаться методом проб и ошибок (trial and error), предпринимая попытки решить задачу, взаимодействуя с окружающим миром, и как-то улучшая своё поведение на основе полученного в ходе этого взаимодействия опыта.  

В обучении с подкреплением (reinforcement learning, RL) мы хотим построить алгоритм, моделирующий обучение методом проб и ошибок. Вместо получения обучающей выборки на вход такой алгоритм будет взаимодействовать с некоторой средой (environment), окружающим миром, а в роли «разметки» будет выступать награда (reward) — скалярная величина, которая выдаётся после каждого шага взаимодействия со средой и показывает, насколько хорошо алгоритм справляется с поставленной ему задачей. Например, если вы печёте тортики, то за каждый испечённый тортик вы получаете $+1$ , а если вы пытаетесь кататься на велосипеде, то за каждое падение с велосипеда вам прилетает -1.  

Награда не подсказывает, как именно нужно решать задачу и что вообще нужно делать; Награда может быть отложенной во времени (вы нашли в пустыне сокровища, но чтобы получить заслуженные тортики, вам ещё понадобится куча времени, чтобы выбраться из пустыни; а награда приходит только за тортики) или сильно разреженной (большую часть времени давать агенту $+0$ ). Всё это сильно отличает задачу от обучения с учителем; Награда предоставляет какой-то «сигнал» для обучения (хорошо/плохо), которого нет, например, в обучении без учителя.  

![](images/d0f474ca25be1c278d43ecb12188a68e50c212944d13669312803aa6ccb1b0df.jpg)  
источник картинки — курс UC Berkeley AI  

# Постановка задачи  

Теперь попробуем формализовать всю эту концепцию и познакомиться с местной терминологией. Задача обучения с подкреплением задаётся Марковским Процессом Принятия Решений (Markov Decision Process или сокращённо MDP) это четвёрка  $\left(\boldsymbol{\mathcal{S}},\boldsymbol{\mathcal{A}},\boldsymbol{\mathcal{P}},\boldsymbol{r} \right)$, где:  

S — пространство состояний (state space), множество состояний, в которых в каждый момент времени может находиться среда.   
A — пространство действий (action space), множество вариантов, из которых нужно производить выбор на каждом шаге своего взаимодействия со средой. $\mathcal{P}$ — функция переходов (transition function), которая задаёт изменение среды после того, как в состоянии $s\in$ $s$ было выбрано действие $a\in$ $\mathcal{A}$ В общем случае функция переходов может быть стохастична, и тогда такая функция переходов моделируется распределением $p(s^{\prime}\mid s,a)$ : с какой вероятностью в какое состояние перейдёт среда после выбора действия $a$ в состоянии $s$ .  

$r{:}S\times$ $A\rightarrow\mathbb{R}$ — функция награды (reward function), выдающая скалярную величину за выбор действия $a$ в состоянии s. Это наш «обучающий сигнал».  

Традиционно субъект, взаимодействующий со средой и влияющий на неё, называется в обучении с подкреплением агентом (agent). Агент руководствуется некоторым правилом, возможно, тоже стохастичным, как выбирать действия в зависимости от текущего состояния среды, которое называется стратегией (policy; термин часто транслитерируют и говорят политика) и моделируется распределением $\pi(a|\quad s)$ . Стратегия и будет нашим объектом поиска, поэтому, как и в классическом машинном обучении, мы ищем какую-то функцию.  

Взаимодействие со средой агента со стратегией $\pi(a|\quad s)$ моделируется так. Изначально среда находится в некотором состоянии $s_{0}$ . Агент сэмплирует действие из своей стратегии $a_{0}\sim$ $\pi(a_{0}\mid s_{0})$ . Среда отвечает на это, сэмплируя своё следующее состояние $s_{1}\sim p\left(s_{1}\mid s_{0},a_{0}\right)\scriptscriptstyle{\boldsymbol{u}\boldsymbol{3}}$ функции переходов, а также выдаёт агенту награду в размере $r(s_{0},a_{0})$ . Процесс повторяется: агент снова сэмплирует $a_{1}$ ​, а среда отвечает генерацией $s_{2}$ ​ и скалярной наградой $r(s_{1},a_{1})$ . Так продолжается до бесконечности или пока среда не перейдёт в терминальное состояние, после попадания в которое взаимодействие прерывается, и сбор агентом награды заканчивается. Если в среде есть терминальные состояния, одна итерация взаимодействия от начального состояния до попадания в терминальное состояние называется эпизодом (episode). Цепочка генерируемых в ходе взаимодействия случайных величин $s_{0}$ ​, $a_{0}$ ​, $s_{1}$ ​, $a_{1}$ ​, $s_{2}$ ​, $a_{2}$ ,… называется траекторией (trajectory). Примечание: функция награды тоже может быть стохастичной, и тогда награды за шаг тоже будут случайными величинами и частью траекторий, но без ограничения общности мы будем рассматривать детерминированные функции награды.  

![](images/a70f06c857f6229545ccb1c99a6a8f9d583b743c9e041c9a236caaccdb2a2564.jpg)  

Итак, $\Phi$ актически среда для нас — это управляемая марковская цепь: на каждом шаге мы выбором $a$ определяем то распределение, из которого будет генерироваться следующее состояние. Мы предполагаем, во-первых, марковское свойство: что переход в следующее состояние определяется лишь текущим состоянием и не зависит от всей предыдущей истории:  

$$
p(s_{t+1}\mid s_{t},a_{t},s_{t-1},a_{t-1},\ldots,s_{0},a_{0})=p(s_{t+1}\mid s_{t},a_{t})
$$  

Во-вторых, мы предполагаем стационарность: функция переходов $p(s^{\prime}\mid s,a)$ не зависит от времени, от того, сколько шагов прошло с начала взаимодействия. Это довольно реалистичные предположения: законы мира не изменяются со временем (стационарность), а состояние — описывает мир целиком (марковость). В этой модели взаимодействия есть только одно нереалистичное допущение: полная наблюдаемость (full observability), которая гласит, что агент в своей стратегии $\pi(a|\quad s)$ наблюдает всё состояние s полностью и может выбирать действия, зная об окружающем мире абсолютно всё; в реальности нам же доступны лишь какие-то частичные наблюдения состояния. Такая более реалистичная ситуация моделируется в частично наблюдаемых MDP (Partially observable MDP, PoMDP), но мы далее ограничимся полностью наблюдаемыми средами.  

Итак, мы научились на математическом языке моделировать среду, агента и их взаимодействие между собой. Осталось понять, чего же мы хотим. Во время взаимодействия на каждом шаге агенту приходит награда $r_{t}=r(s_{t},a_{t})$ , однако, состояния и действия $s_{t}$ ​, $a_{t}$ в рамках такой постановки — случайные величины. Один и тот же агент может в силу стохастики как внутренней (в силу случайности выбора действий в его стратегии), так и внешней (в силу стохастики в функции переходов) набирать очень разную суммарную награду $\textstyle\sum_{t\geq0}r_{t}$ в зависимости от везения. Мы скажем, что хотим научиться выбирать действия так, чтобы собирать в среднем как можно больше награды.  

Что значит в среднем, в среднем по чему? По всей стохастике, которая заложена в нашем процессе взаимодействия со средой. Каждая стратегия $\pi$ задаёт распределение в пространстве траекторий — с какой вероятностью нам может встретится траектория $\tau=$ $\left(s_{0},a_{0},s_{1},a_{1},\dots\right)$ :  

$$
p({\mathcal T}|{\textit{\pi}})=p(s_{0},a_{0},s_{1},a_{1},\cdot\cdot\cdot|{\textit{\pi}})=\prod_{t\geq0}p(s_{t+1}|s_{t},a_{t}){\pi}(a_{t}|s_{t})
$$  

Вот по такому распределению мы и хотим взять среднее получаемой агентом награды. Записывают это обычно как-нибудь так:  

$$
\mathbb{E}_{T\sim\pi}\sum_{t\geq0}r_{t}\to\operatorname*{max}_{\pi}
$$  

Здесь мат.ожидание по траекториям — это бесконечная цепочка вложенных мат.ожиданий:  

$$
\mathbb{E}_{T\sim\pi}(\cdot)=\mathbb{E}_{a_{0}\sim\pi(a_{0}\mid s_{0})}\mathbb{E}_{s_{1}\sim p(s_{1}\mid s_{0},a_{0})}\mathbb{E}_{a_{1}\sim\pi(a_{1}\mid s_{1})}\dots(\cdot)
$$  

Вот такую конструкцию мы и хотим оптимизировать выбором стратегии $\pi$ . На практике, однако, вносят ещё одну маленькую корректировку. В средах, где взаимодействие может продолжаться бесконечно долго, агент может научиться набирать бесконечную награду, с чем могут быть связаны разные парадоксы (например, получать $+1$ на каждом втором шаге становится также хорошо, как получать $+1$ на каждом сотом шаге). Поэтому вводят дисконтирование (discounting) награды, которое гласит: тортик сейчас лучше, чем тот же  

самый тортик завтра. Награду, которую мы получим в будущем, агент будет дисконтировать на некоторое число $\gamma$ , меньшее единицы. Тогда наш функционал примет такой вид:  

$$
\mathbb{E}_{T\sim\pi}\sum_{t\geq0}\gamma^{t}r_{t}\to\operatorname*{max}_{\pi}
$$  

![](images/ee89d39c3619039aeb8b6ee15ea83fc198582335fd617f049e7bd8a03a9ccda6.jpg)  
источник картинки — курс UC Berkeley AI  

Заметим, что обучение с подкреплением - это в первую очередь задача оптимизации, оптимизации функционалов определённого вида. Если в классическом машинном обучении подбор функции потерь можно считать элементом инженерной части решения, то здесь функция награды задана нам готовая и определяет тот функционал, который мы хотим оптимизировать.  

# Примеры  

Формализм MDP очень общий, и под него попадает практически всё, что можно назвать «интеллектуальной задачей» (с той оговоркой, что не всегда очевидно, какая функция награды задаёт ту или иную задачу).  

Самые простые примеры MDP можно нарисовать «на бумажке». Например, часто рассматривают «клетчатые миры» (GridWorlds): агент находится в некоторой позиции клетчатой доски и может в качестве действий выбирать одно из четырёх направлений. Такие миры могут по-разному реагировать агента за выбор действия «пойти в стену», с некоторой вероятностью перемещать агента не в том направлении, которое он выбрал, содержать предметы в некоторых клетках и так далее. Пространство состояний, в которых может оказаться агент, в таких примерах конечно, как и пространство действий. Такие MDP называют табличными: все состояния и действия можно перечислить.  

![](images/1e062fa68024bb497fa5eef0a680e2f63669bf82f497dae426959e29ecb57a37.jpg)  
источник картинки — курс UC Berkeley AI  

Огромное разнообразие MDP предоставляют видеоигры. Можно считать, что на вход агенту подаётся изображение экрана видеоигры, и несколько раз в секунду агент выбирает, какие кнопки на контроллере он хочет нажать. Тогда пространство состояний - множество всевозможных картинок, которые вам может показать видеоигра. Множество, в общем-то, конечное (конечное количество пикселей экрана с тремя цветовыми каналами, каждый из который показывает целочисленное значение от 0 до 255), но только очень большое; например, их уже нельзя перечислить или сохранить все возможные варианты в памяти. Но на каждом шаге нужно выбирать действие из конечного набора: какие кнопки нажать, поэтому это задачи дискретного управления.  

![](images/e590a4f7966ec2627b3c0037d613d81d0f1351b4d40707d6905679ab73a9da5a.jpg)  

# источник картинки — курс UC Berkeley AI  

Наконец, естественный способ создавать среды — использование физических симуляций. В качестве бенчмарка часто используют locomotion — задачу научить какое- нибудь «существо» ходить в рамках той или иной физической модели (примеры можно посмотреть, например, здесь). Причём концептуально, в рамках задачи обучения с подкреплением, нам даже неважно, как именно устроена симуляция или как задана функция награды: мы хотим построить общий алгоритм оптимизации этой самой награды. Если награда поощряет перемещение центра масс «существа» вдоль некоторого направления, агент постепенно научится выбирать действия так, чтобы существо перемещалось и не падало, если последнее приводит к завершению эпизода и мешает дальнейшему получению награды.  

![](images/5d0bf19a3fb6829475d535735516357593e60071be9780215b38ffad49dc588a.jpg)  
источник картинки — статья DeepMind Producing Flexible Behaviours in Simulated Environments  

В таких задачах агент на каждом шаге выбирает несколько вещественных чисел в диапазоне [−1,1 ], где -1 — «максимально расслабить» сустав, а $+1$ — «максимально напрячь». Такое пространство действий возникает во многих задачах робототехники, где нужно научиться поворачивать какой-нибудь руль, и у него есть крайнее правое и крайнее левое положение, но можно выбрать и любое промежуточное. Такие задачи называются задачами непрерывного управления (continuous control).  

# Окей, и как такое решать?  

Выглядит сложновато, но у человечества есть уже довольно много наработок, как подойти к этой на вид очень общей задаче, причём с основной идеей вы скорее всего уже сталкивались. Называется она динамическим программированием.  

Дело в том, что мы оптимизируем не абы какой функционал, а среднюю дисконтированную кумулятивную награду. Чтобы придумать более эффективное решение, чем какой-нибудь подход, не использующий этот факт (например, эволюционные алгоритмы), нам нужно воспользоваться структурой поставленной задачи. Эта структура задана в формализме MDP и определении процесса взаимодействия агента со средой. Интуитивно она выражается так: вот мы сидим в некотором состоянии s и хотим выбрать действие $a$ как можно оптимальнее. Мы знаем, что после выбора этого действия мы получим награду за этот шаг $r=r(s,a)$ , среда перекинет нас в состояние s и, внимание, дальше нас ждёт подзадача эквивалентной структуры: в точности та же задача выбора оптимального действия, только в другом состоянии. Действительно: когда мы будем принимать решение на следующем шаге, на прошлое мы повлиять уже не способны; стационарность означает, что законы, по которым ведёт себя среда, не поменялись, а марковость говорит, что история не влияет на дальнейший процесс нашего взаимодействия. Это наводит на мысль, что задача максимизации награды из текущего состояния тесно связана с задачей максимизации награды из следующего состояния $s^{\prime}$ , каким бы оно ни было.  

Чтобы сформулировать это на языке математики, вводятся «дополнительные переменные», вспомогательные величины, называемые оценочными функциями. Познакомимся с одной такой оценочной функцией - оптимальной Q-функцией, которую будем обозначать $Q^{*}(s,a)$ . Скажем, что $Q^{*}(s,a)$ - это то, сколько максимально награды можно (в среднем) набрать после выбора действия $a$ из состояния s. Итак:  

$$
Q^{*}(s,a)=\operatorname*{max}_{\pi}\mathbb{E}_{\mathcal{T}\sim\pi\mid s_{0}=s,a_{0}=a}\sum_{t\geq0}\gamma^{t}r_{t}
$$  

Запись ${\mathcal{T}}\sim\pi|~s_{0}=s,a_{0}=a$ здесь означает, что мы садимся в состояние $s_{0}=s$ ; выбираем действие $a_{0}=a$ , а затем продолжаем взаимодействие со средой при помощи стратегии $\pi$ , порождая таким образом траекторию T . По определению, чтобы посчитать $Q^{*}(s,a)$ , нужно перебрать все стратегии, посмотреть, сколько каждая из них набирает награды после выбора $a$ из состояния s, и взять наилучшую стратегию. Поэтому эта оценочная функция называется оптимальной: она предполагает, что в будущем после выбора действия $a$ из состояния $s$ агент будет вести себя оптимально.  

Определение неконструктивное, конечно, поскольку в реальности мы так сделать не можем, зато обладает интересным свойством. Если мы каким-то чудом узнали $Q^{*}(s,a)$ , то мы знаем оптимальную стратегию. Действительно: представьте, что вы находитесь в состоянии $s$ , вам нужно сделать выбор из трёх действий, и вы знаете значения $Q^{*}(s,a)$ . Вы знаете, что если выберете первое действие $a=0$ , то в будущем сможете набрать не более чем, допустим, $Q^{*}(s,a=0)=+3$ награды. При этом вы знаете, что существует какая-то стратегия $\pi$ , на которой достигается максимум в определении оптимальной Q-функции, то есть которая действительно позволяет набрать эти $+3.$ . Вы знаете, что если выберете второе действие, то в будущем сможете набрать, допустим, $Q^{*}(s,a=1)=+10$ , а для третьего действия $Q^{*}(s,a=2)=-1.$ Вопрос: так как нужно действовать? Интуиция подсказывает, что надо просто выбирать действие $a=1$ , что позволит набрать $+10,$ , ведь по определению больше набрать никак не получится. Значит, выбор в этом состоянии действия $a=1$ оптимален. Эта интуиция нас не обманывает, и принцип такого выбора называется принципом оптимальност Беллмана.  

Выбор того действия, на котором достигается максимум по действиям Q-функции, называется жадным (greedy) по отношению к ней. Таким образом, принцип оптимальности Беллмана гласит:  

жадный выбор по отношению к оптимальной Q-функции оптимален:  

$$
\pi^{*}(s)=\operatorname{argmax}_{a}Q^{*}(s,a)
$$  

Примечание: если Q-функция достигает максимума на нескольких действиях, то можно выбирать любое из них.  

Заметим, что эта оптимальная стратегия детерминирована. Этот интересный факт означает, что нам, в общем-то, необязательно искать стохастичную стратегию. Наше рассуждение пока даже показывает, что мы можем просто пытаться найти $Q^{*}(s,a)$ , а дальше выводить из неё оптимальную стратегию, выбирая действие жадно.  

Но как искать $Q^{*}(s,a)$ ? Тут на сцене и появляется наше наблюдение про структуру задачи. Оказывается, $Q^{*}(s,a)$ выражается через саму себя. Действительно: рассмотрим некоторую пару состояние-действие $s,a$ . С одной стороны, по определению, мы в будущем сможем при условии оптимального поведения получить $Q^{*}(s,a)$ награды. С другой стороны, после того, как мы выберем действие $a$ в состоянии $s$ , мы получим награду за один шаг $\boldsymbol{r}(s,a)$ , вся дальнейшая награда будет дисконтирована на $\gamma$ , среда ответит нам сэмплированием $\mathbf{\boldsymbol{s}}^{\prime}\sim p(\mathbf{\boldsymbol{s}}^{\prime}\mid\mathbf{\boldsymbol{s}},\boldsymbol{a})$ (на результат этого сэмплирования мы уже никак повлиять не можем и по этой стохастике нашу будущую награду надо будет усреднять), а затем в состоянии $s^{\prime}$ мы, в предположении оптимальности поведения, выберем то действие $a^{\prime}$ , на котором достигается максимум $Q^{*}(s^{\prime},a^{\prime})$ . Другими словами, в дальнейшем после попадания в $s^{\prime}$ мы сможем получить $\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime})$ награды. А значит, верно следующее рекурсивное соотношение, называемое уравнением оптимальности Беллмана для Q-функции:  

$$
Q^{*}(s,a)=r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime})
$$  

Мы получили систему уравнений, связывающую значения $Q^{*}(s,a)$ с самой собой. Это нелинейная система уравнений, но оказывается, что она в некотором смысле «хорошая». У неё единственное решение - и, значит, решение этого уравнения можно считать эквивалентным определением $Q^{*}(s,a)$ , - и его можно искать методом простой итерации. Метод простой итерации решения систем уравнений позволяет улучшать своё текущее приближение $x$ решения некоторого уравнения вида $x=f(x)$ его подстановкой в правую часть. То есть: инициализируем произвольную функцию $\begin{array}{r l}{Q_{0}^{*}(s,a)\colon S\times}&{{}A\to\mathbb{R},}\end{array}$ , которая будет приближать $Q^{*}(s,a)$ , затем итеративно будем подставлять её в правую часть уравнений оптимальности Беллмана и полученным значением обновлять наше приближение:  

$$
\begin{array}{r l}{Q_{k+1}^{*}(s,a)\gets}&{{}r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})}\end{array}
$$  

Такая процедура в пределе приведёт нас к истинной $Q^{*}(s,a)$ , а значит и оптимальной стратегии. Кстати, когда вы в прошлом встречались с динамическим программированием, вы скорее всего неявно использовали именно эту идею, разве что часто в задачах для решения уравнений оптимальности Беллмана можно просто последовательно исключать неизвестные переменные; но метод простой итерации даёт более общую схему, применимую всегда. А сейчас для нас принципиально следующее: если у нас есть какое-то приближение $Q^{*}$ , то вычисление правой части уравнения оптимальности Беллмана позволит получить приближение лучше.  

# А где же метод проб и ошибок?  

Решать методом простой итерации уравнения оптимальности Беллмана и таким образом получать $Q^{*}(s,a)$ в реальности можно только при двух очень существенных ограничивающих условиях. Нужно, чтобы, во-первых, мы могли хранить как-то текущее приближение $Q_{k}^{*}(s,a){\bf\delta B}$ памяти. Это возможно только если пространства состояний и действий конечные и не очень большие, то есть, например, в вашем MDP всего 10 состояний и 5 действий, тогда $Q^{*}(s,a)-\mathsf{s T o}$ табличка 10x5. Но что, если вы хотите научиться играть в видеоигру, и состояние — это входное изображение? Тогда множество картинок, которые вам может показать видеоигра, сохранить в памяти уже не получится. Ну, допустим пока, что число состояний и число действий не очень большое, и мы всё-таки можем хранить таблицу в памяти, а позже мы снимем это ограничение, моделируя $Q^{*}(s,a)$ при помощи нейросети.  

Во-вторых, нам необходимо уметь считать выражение, стоящее справа в уравнение оптимальности Беллмана:  

$$
r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})
$$  

Мало того, что в сложных средах взять мат.ожидание по функции переходов $\mathbb{E}_{s^{\prime}\sim p\left(s^{\prime}\mid s,a\right)}$ в реальности мы не сможем, так ещё и обычно мы эту функцию переходов на самом деле не знаем. Представьте, что вы катаетесь на велосипеде: можете ли вы по текущему состоянию окружающего мира, например, положению всех атомов во вселенной, рассказать, с какими вероятностями в каком состоянии мир окажется в следующий момент времени? Это соображение также подсказывает, что было бы здорово, если б мы смогли решать задачу, избегая даже попыток выучить эту сложную функцию переходов.  

Что нам доступно? Мы можем взять какую-нибудь стратегию $\pi$ (важный момент: мы должны сами выбрать какую) и повзаимодействовать ею со средой. «Попробовать решить задачу». Мы можем сгенерировать при помощи $\pi$ целую траекторию или даже сделать всего один шаг в среде. Таким образом мы соберём данные: допустим, мы были в состоянии s и сделали выбор действия $a$ , тогда мы узнаем, какую награду $r=r(s,a)$ мы получаем за такой шаг и, самое главное, в какое состояние $s^{\prime}$ нас перевела среда. Полученный $s^{\prime}$ — сэмпл из функции переходов $\mathbf{\boldsymbol{s}}^{\prime}\sim p(\mathbf{\boldsymbol{s}}^{\prime}\mid\mathbf{\boldsymbol{s}},\boldsymbol{a})$ . Собранная так информация — четвёрка $\left(s,a,r,s^{\prime}\right)-$ называется переходом (transition), и может быть как-то использована для оптимизации нашей стратегии. Можем ли мы, используя лишь переходы $\left(s,a,r,s^{\prime}\right)$ , то есть имея на руках лишь сэмплы $s^{\prime}\sim$ $p(s^{\prime}\mid s,a)$ , как-то пользоваться схемой динамического программирования? Что, если мы будем заменять значение $Q_{k}^{*}(s,a)$ не на  

$$
r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime}),
$$  

которое мы не можем посчитать, а на его Монте Карло оценку:  

$$
r(s,a)+\gamma\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime}),
$$  

где $s^{\prime}$ — сэмпл из функции переходов из собранного нами опыта? В среднем-то такая замена верная. Такая Монте-Карло оценка правой части для заданного переходика $\left(s,a,r,s^{\prime}\right)$ называется Беллмановским таргетом, то есть «целевой переменной». Почему такое название — мы увидим чуть позже.  

Чтобы понять, как нам нужно действовать, рассмотрим какую-нибудь типичную ситуацию. Допустим, после выполнения действия $a$ из некоторого состояния $s$ среда награждает нас $r(s,a)=0$ и перекидывает нас с равными вероятностями то в состояние $s^{\prime}$ , для которого $\begin{array}{r}{\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})=+1,}\end{array}$ , то в состояние $s^{\prime}$ , для которого $\begin{array}{r}{\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})=-1.}\end{array}$ Метод простой итерации говорит, что на очередной итерации нужно заменить $Q_{k}^{*}(s,a)\mathsf{H a}0.5\gamma$ ⋅ $(+1)+$ $0.5\gamma$ ⋅ $(-1)=0$ , но в реальности мы встретимся лишь с одним исходом, и таргет — Монте- Карло оценка правой части уравнения оптимальности Беллмана — будет с вероятностью 0.5 равен $+\gamma$ , а с вероятностью 0.5 равен $-\gamma$ . Ясно, что нельзя просто взять и жёстко заменять наше текущее приближение $Q_{k}^{*}(s,a)$ на посчитанный Беллмановский таргет по некоторому одному переходу, поскольку нам могло повезти (мы увидели $+\gamma$ ) или не повезти (мы увидели $-\gamma)$ . Давайте вместо этого поступать также, как учат среднее по выборке: не сдвигать «жёстко» наше текущее приближение в значение очередного сэмпла, а смешивать текущее приближение с очередным сэмплом. То есть: берём переходик $\left(s,a,r,s^{\prime}\right)$ , и не заменяем $Q_{k}^{*}(s,a)$ на стохастичную оценку правой части уравнения оптимальности Беллмана, а только сдвигаемся в его сторону:  

$$
Q_{k+1}^{*}(s,a)\gets~(1-\alpha)Q_{k}^{*}(s,a)+\alpha(r+\gamma\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime}))
$$  

Таким образом, мы проводим экспоненциальное сглаживание старого приближения $Q_{k}^{*}(s,a)$ и новой оценки правой части уравнения оптимальности Беллмана со свежим сэмплом $s^{\prime}$ . Выбор $\alpha$ здесь определяет, насколько сильно мы обращаем внимание на последние сэмплы, и имеет тот же физический смысл, что и learning rate. В среднем по стохастике (а стохастика в этой формуле обновления заложена в случайном $s^{\prime}$ ) мы будем сдвигаться в сторону  

$$
r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime}),
$$  

и значит применять этакий «зашумлённый» метод простой итерации.  

Итак, возникает следующая идея. Будем как-то взаимодействовать со средой и собирать переходики $\left(s,a,r,s^{\prime}\right)$ . Для каждого перехода будем обновлять одну ячейку нашей Q-таблицы размера число состояний на число действий по вышеуказанной формуле. Таким образом мы получим как бы «зашумлённый» метод простой итерации, где мы на каждом шаге обновляем только одну ячейку таблицы, и не заменяем жёстко значение на правую часть уравнений  

оптимальности, а лишь сдвигаемся по некоторому в среднем верному стохастичному направлению.  

Очень похоже на стохастическую оптимизацию вроде стохастического градиентного спуска, и поэтому гарантии сходимости выглядят схожим образом. Оказывается, такой алгоритм сходится к истинной $Q^{*}(s,a)$ , если для любой пары $s,a$ мы в ходе всего процесса проводим бесконечное количество обновлений, а learning rate (гиперпараметр $\alpha$ ) в них ведёт себя как learning rate из условий сходимости стохастического градиентного спуска:  

$$
\sum_{i}\alpha_{i}=+\infty,\qquad\sum_{i}\alpha_{i}^{2}<+\infty
$$  

# Пример  

Колобок Колабулька любит играть в лотереи. Допустим, в некотором состоянии s он выполнил действие $a$ «купить билет» и предполагает, что в будущем сможет набрать $Q^{*}(s,a)=+100$ награды. Однако, за покупку билета он платит 10 долларов, и таким образом теряет 10 награды на данном шаге $r(s,a)=-10$ , при этом попадая в состояние $s^{\prime}=s$ , где ему снова предлагается купить билет в лотерею (он может выбрать действие «купить» или действие «не купить»). Ну, допустим, текущая оценка Колабульки будущей награды в случае отказа купить билет равна $0<+100$ , поэтому колобок предполагает, что в будущем из состояния $s^{\prime}$ он сможет получить m $\arg Q^{*}(s^{\prime},a^{\prime})=+100.$ . Для простоты допустим $\gamma=+1$ . Тогда получается, что одношаговое приближение будущей награды $r(s,a)+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime})=-10+100=90.$ Да, $s^{\prime}$ здесь — случайная величина, колобку могло повезти или не повезти (и мы, увидев всего один сэмпл из функции переходов, не можем сказать наверняка, повезло ли нам сейчас или нет), но наша $\Phi$ ормула говорит сдвинуть аппроксимацию $Q^{*}(s,a)$ в сторону Беллмановского таргета.  

$\begin{array}{c c c}{{s\longrightarrow}}&{{\displaystyle{\begin{array}{c}{\circ^{\circ}{\frac{\circ^{\circ}\left(Q^{\star}(s,a)=+100\right)}{a::C^{\circ}8\mathrm{BL}}}}\ {{-}}\end{array}}}}&{{r(s,a)=-10}}\ {{\longrightarrow}}&{{Q^{\star}(s^{\prime},a^{\prime}=40!)=+100}}\ {{Q^{\star}(s^{\prime},a^{\prime}==5\mathrm{B}!)=0}}\end{array}$ $\mathring{\mathfrak{z}^{\star}}(s,a)\longleftarrow(1-\alpha)Q^{\star}(s,a)+\alpha\big(r(s,a)+\mathop{m a x}_{a^{\prime}}Q^{\star}(s^{\prime},a^{\prime})\big)=(1-\alpha)\bullet100$  

Допустим, learning rate $\alpha=0.5$ : тогда, сдвигая +100 в сторону $+90$ , ожидания от будущей награды после покупки лотерейного билета опускаются до 95. Всё ещё $+95>0$ , поэтому колобку кажется, что покупать билет выгоднее, чем не покупать, поэтому рассмотрим следующий переходик. Допустим, колобок снова купил билет, снова потерял 10 долларов и снова попал в то же самое $s^{\prime}=s.$ . Наше обновление снова скажет уменьшать значение $Q^{*}(s,a)$ :  

![](images/46ad0328877815b26d19342967c906411a503ccb5937922bbeeb87b0a2057efc.jpg)  

Видно, что если колобку продолжит так не везти, таргет будет всё время на 10 меньше, чем текущее приближение, и $Q^{*}(s,a)$ будет всё уменьшаться и уменьшаться, пока не свалится до нуля (а там будет выгоднее уже не покупать билет). Но если на очередной итерации Колабульке повезло, и среда перевела его в $s^{\prime}$ , соответствующее победе в лотерею (а это, видимо, происходит с какой-то маленькой вероятностью), таргет получится очень большим, и аппроксимацию $Q^{*}(s,a)$ наше обновление скажет сильно увеличить:  

![](images/d54fdd3121ba7c58b8c1d2b9c85dbcb3a1c63370080eb125e0e17592592357b8.jpg)  

Куда будет сходиться такой алгоритм? Давайте предположим, что среда на покупку лотерейного билета отвечает с вероятностью $p$ возвращением в то же состояние $s^{\prime}=s$ , где колобку предлагается купить ещё один билет, а с вероятностью 1− $p$ билет оказывается выигрышным, и колобок попадает в такое состояние $s^{\prime}$ , в котором он может забрать приз и получить $+1000$ (после этого взаимодействие со средой, скажем, заканчивается). Давайте запишем уравнение оптимальности Беллмана для действия $a$ «купить билет» в состоянии s:  

$$
Q^{*}(s,a)=r(s,a)+\gamma\left(p\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime}=s,a^{\prime})+(1-p){\cdot}(+1000\mathrm{~})\right)
$$  

Здесь $\operatorname*{max}_{a^{\prime}}$ ​ $Q^{*}(s^{\prime}=s,a^{\prime})=\operatorname*{max}(Q^{*}(s,a),0)$ , поскольку колобок может или покупать билет, или не покупать (это, допустим, принесёт ему 0 награды). Понятно, что если покупка билета не принесёт больше 0 награды, то не имеет смысла его покупать. Подставляя все числа из примера, получаем:  

$$
Q^{*}(s,a)=-10+p\mathrm{max}\left(Q^{*}(s,a),0\right)+1000(1-p)
$$  

Видно, что если вероятность проигрыша в лотерею $p=0.99$ , то решением уравнения является $Q^{*}(s,a)=0.$ : Колабулька платит за билет 10 долларов и получает 1000 награды с вероятностью 0.01. В этом случае действие «купить билет» и «не покупать» равноценны, и оба в будущем принесут в среднем 0 награды. Если же $p>0.99$ , то покупать билет становится невыгодно, а если $p<0.99$ , то выгодно покупать билет до тех пор, пока не случится победа. Несмотря на то, что в таргете содержится собственная же текущая аппроксимация будущей награды и используется лишь один сэмпл s вместо честного усреднения по всевозможным исходам, $\Phi$ ормула обновления постепенно сойдётся к этому решению. Причём колобку истинное значение $p$ неизвестно, и в формуле обновления эта вероятность влияла лишь на появление того или иного $s^{\prime}$ в очередном таргете.  

Этот алгоритм, к которому мы уже практически пришли, называется Q-learning, «обучение оптимальной Q-функции». Нам, однако, осталось ответить на один вопрос: так как же нужно собирать данные, чтобы удовлетворить требованиям для сходимости? Как взаимодействовать со средой так, чтобы мы каждую ячейку s,a не прекращали обновлять?  

# Дилемма Exploration-exploitation  

Мы уже встречали дилемму exploration-exploitation (букв. «исследования-использования») в параграфе про тюнинг гиперпараметров. Задача многоруких бандитов, которая там встретилась, на самом деле является частным случаем задачи обучения с подкреплением, в котором после первого выбора действия эпизод гарантированно завершается, и этот частный случай задачи часто используется для изучения этой дилеммы. Рассмотрим эту дилемму в нашем контексте.  

Допустим, на очередном шаге алгоритма у нас есть некоторое приближение $Q_{k}(s,a)\approx$ $Q^{*}(s,a)$ . Приближение это, конечно, неточное, поскольку алгоритм, если и сходится к истинной оптимальной Q-функции, то на бесконечности. Как нужно взаимодействовать со средой? Если вы хотите набрать максимальную награду, наверное, стоит воспользоваться нашей теорией и заниматься exploitation-ом, выбирая действие жадно:  

$$
\pi(s)=\operatorname{argmax}_{a}Q_{k}(s,a)
$$  

Увы, такой выбор не факт что совпадёт с истинной оптимальной стратегией, а главное, он детерминирован. Это значит, что при взаимодействии этой стратегией со средой, многие пары s,a никогда не будут встречаться просто потому, что мы никогда не выбираем действие $a$ в состоянии s. А тогда мы, получается, рискуем больше никогда не обновить ячейку $Q_{k}(s,a)$ для таких пар!  

Такие ситуации запросто могут привести к застреванию алгоритма. Мы хотели научиться кататься на велосипеде и получали $+0.1$ за каждый пройденный метр и -5 за каждое попадание в дерево. После первых проб и ошибок мы обнаружили, что катание на велосипеде приносит нам -5, поскольку мы очень скоро врезаемся в деревья и обновляли нашу аппроксимацию Qфункции сэмплами с негативной наградой; зато если мы не будем даже забираться на велосипед и просто займёмся ничего не деланьем, то мы сможем избежать деревьев и будем получать 0. Просто из-за того, что в нашей стратегии взаимодействия со средой никогда не встречались те $s,a$ , которые приводят к положительной награде, и жадная стратегия по отношению к нашей текущей аппроксимации Q-функции никогда не выбирает их. Поэтому нам нужно экспериментировать и пробовать новые варианты.  

Режим exploration-а предполагает, что мы взаимодействуем со средой при помощи какой- нибудь стохастичной стратегии $\forall s,a\colon\pi(a|\quad s)>0.$ Например, такой стратегией является случайная стратегия, выбирающая рандомные действия. Как ни странно, сбор опыта при помощи случайной стратегии позволяет побывать с ненулевой вероятностью во всех областях пространства состояний, и теоретически даже наш алгоритм обучения Q-функции будет сходится. Означает ли это, что exploration-а хватит, и на exploitation можно забить?  

В реальности мы понимаем, что добраться до самых интересных областей пространства состояний, где функция награда самая большая, не так-то просто, и случайная стратегия хоть и будет это делать с ненулевой вероятностью, но вероятность эта будет экспоненциально маленькая. А для сходимости нам нужно обновить ячейки $Q_{k}(s,a)$ для этих интересных состояний бесконечно много раз, то есть нам придётся дожидаться необычайно редкого везения далеко не один раз. Куда разумнее использовать уже имеющиеся знания и при помощи жадной стратегии, которая уже что-то умеет, идти к этим интересным состояниям. Поэтому для решения дилеммы exploration-exploitation обычно берут нашу текущую жадную стратегию и что-нибудь с ней делают такое, чтобы она стала чуть-чуть случайной. Например, с вероятностью $\varepsilon>0$ выбирают случайное действие, а с вероятностью $1-$ ε — жадное. Тогда мы чаще всё- таки и знаниями пользуемся, и любое действие с ненулевой вероятностью выбираем; такая стратегия называется $\varepsilon$ -жадной, и она является самым простым способом как-то порешать эту дилемму.  

Давайте закрепим, что у нас получилось, в виде табличного алгоритма обучения с подкреплением под названием Q-learning:  

1. Проинициализировать $Q^{*}(s,a)$ произвольным образом.   
2. Пронаблюдать $s_{0}$ ​ из среды.   
3. Для $k=0,1,2,$ … :  

с вероятностью $\varepsilon$ выбрать действие $a_{k}$ ​ случайно, иначе жадно: $a_{k}=\mathrm{argmax}_{a_{k}}Q^{*}(s_{k},a_{k})$   
отправить действие $a_{k}$ ​ в среду, получить награду за шаг $r_{k}$ ​ и следующее состояние $s_{k+1}$ ​.   
обновить одну ячейку таблицы:  

$$
Q^{*}(s_{k},a_{k})\gets(1-\alpha)Q^{*}(s_{k},a_{k})+\alpha(r_{k}+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s_{k+1},a^{\prime}))
$$  

# Добавим нейросеток  

Наконец, чтобы перейти к алгоритмам, способным на обучение в сложных MDP со сложным пространством состояний, нужно объединять классическую теорию обучения с подкреплением с парадигмами глубокого обучения.  

Допустим, мы не можем позволить себе хранить $Q^{*}(s,a)$ как таблицу в памяти, например, если мы играем в видеоигру и на вход нам подаются какие-нибудь изображения. Тогда мы можем обрабатывать любые имеющиеся у агента входные сигналы при помощи нейросетки $Q^{*}(s,a,\theta)$ . Для тех же видеоигр мы легко обработаем изображение экрана небольшой свёрточной сеточкой и выдадим для каждого возможного действия $a$ вещественный скаляр $Q^{*}(s,a,\theta)$ . Допустим также, что пространство действий всё ещё конечное и маленькое, чтобы мы могли для такой модели строить жадную стратегию, выбирать argmaxa​ $Q^{*}(s,a,\theta)$ . Но как обучать такую нейросетку?  

Давайте ещё раз посмотрим на формулу обновления в Q-learning для одного переходика $\left(s,a,r,s^{\prime}\right)$ :  

$$
\begin{array}{r l r}&{}&{Q_{k+1}^{*}(s,a)\gets(1-\alpha\mathrm{\boldmath~\Psi~})Q_{k}^{*}(s,a)+\alpha(r+\gamma\mathrm{\boldmath~\Psi~}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime}))=}\ &{}&{=Q_{k}^{*}(s,a)+\alpha(r+\gamma\mathrm{\boldmath~\Psi~}\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})-Q_{k}^{*}(s,a))}\end{array}
$$  

Теория Q-learning-а подсказывала, что у процесса такого обучения Q-функции много общего с обычным стохастическим градиентным спуском. В таком виде формула подсказывает, что, видимо,  

$$
r+\gamma\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})-Q_{k}^{*}(s,a)
$$  

— это стохастическая оценка какого-то градиента. Этот градиент сравнивает Беллмановский таргет  

$$
r+\gamma\operatorname*{max}_{a^{\prime}}Q_{k}^{*}(s^{\prime},a^{\prime})
$$  

с нашим текущим приближением $Q_{k}^{*}(s,a)\mid$ и чуть-чуть корректирует это значение, сдвигая в сторону таргета. Попробуем «заменить» в этой $\Phi$ ормуле Q-функцию с табличного представления на нейросетку.  

Рассмотрим такую задачу регрессии. Чтобы построить один прецедент для обучающей выборки, возьмём один имеющийся у нас переходик $\left(s,a,r,s^{\prime}\right)$ . Входом будет пара $s,a$ . Целевой переменной, таргетом, будет Беллмановский таргет  

$$
y=r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta);
$$  

его зависимость от параметров $\theta$ нашей нейронки мы далее будем игнорировать и будем «притворяться», что это и есть наш ground truth. Именно поэтому Монте-Карло оценка правой части уравнения оптимальности Беллмана и называют таргетом. Но важно помнить, что эта целевая переменная на самом деле «зашумлена»: в формуле используется взятый из перехода $s^{\prime}$ , который есть лишь сэмпл из функции переходов. На самом же деле мы хотели бы выучить среднее значение такой целевой переменной, и поэтому в качестве функции потерь мы возьмём MSE. Как будет выглядеть шаг стохастического градиентного спуска для решения этой задачи регрессии (для простоты — для одного прецедента)?  

$$
\begin{array}{r l}&{\partial_{k+1}\leftarrow\theta_{k}-\alpha\nabla_{\theta}(y-\mathrm{\bf~Q}^{*}(s,a,\theta))^{2}=}\ &{\qquad=\theta_{k}+2\alpha(y-\mathrm{\bf~}Q^{*}(s,a,\theta))\nabla_{\theta}Q^{*}(s,a,\theta)=}\ &{\qquad=\theta_{k}+2\alpha(r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)-\mathrm{\bf~}Q^{*}(s,a,\theta))\nabla_{\theta}Q^{*}(s,a,\theta)}\end{array}
$$  

Это практически в точности повторяет $\Phi$ ормулу Q-learning, которая гласит, что если таргет $r+$ $\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)$ больше $Q^{*}(s,a,\theta)$ , то нужно подстроить веса нашей модели так, чтобы $Q^{*}(s,a,\theta)$ стало чуть побольше, и наоборот. В среднем при такой оптимизации мы будем двигаться в сторону  

$$
\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}y=\mathbb{E}_{s^{\prime}\sim p(s^{\prime}|s,a)}\left[r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)\right]
$$  

— в сторону правой части уравнения оптимальности Беллмана, то есть моделировать метод простой итерации для решения системы нелинейных уравнений.  

Единственное отличие такой задачи регрессии от тех, с которыми сталкивается традиционное глубокое обучение — то, что целевая переменная зависит от нашей же собственной модели. Раньше целевые переменные были напрямую источником обучающего сигнала. Теперь же, когда мы хотим выучить будущую награду при условии оптимального поведения, мы не знаем этого истинного значения или даже её стохастичных оценок. Поэтому мы применяем идею бутстрапирования (bootstrapping): берём награду за следующий шаг, и нечестно приближаем всю остальную награду нашей же текущей аппроксимацией $\operatorname*{max}_{a^{\prime}}$ $Q^{*}(s^{\prime},a^{\prime},\theta)$ . Да, за этим кроется идея метода простой итерации, но важно понимать, что такая целевая переменная лишь указывает направление для обучения, но не является истинным приближением будущих наград или даже их несмещённой оценкой. Поэтому говорят, что в этой задаче регрессии очень смещённые (biased) целевые переменные.  

На практике из-за этого возникает беда. Наша задача регрессии в таком виде меняется после каждого же шага. Если вдруг после очередного шага оптимизации и обновления весов нейросети наша модель начала выдавать какие-то немного неадекватные значения, они рискуют попасть в целевую переменную на следующем шаге, мы сделаем шаг обучения под неадекватные целевые переменные, модель станет ещё хуже, и так далее, начнётся цепная реакция. Алгоритмы, в которых целевая переменная вот так напрямую зависит от текущей же модели, из-за этого страшно нестабильны.  

Для стабилизации применяется трюк, называемый таргет-сетью (target network). Давайте сделаем так, чтобы у нас задача регрессии менялась не после каждого обновления весов нейросетки, а хотя бы раз, скажем, в 1000 шагов оптимизации. Для этого заведём полную копию нашей нейросети («таргет-сеть»), веса которой будем обозначать $\theta^{-}$ . Каждые 1000 шагов будем копировать веса из нашей модели в таргет-сеть $\theta^{-}\leftarrow\theta$ , больше никак менять $\theta^{-}$ не будем. Когда мы захотим для очередного перехода $\left(s,a,r,s^{\prime}\right)$ построить таргет, мы воспользуемся не нашей свежей моделью, а таргет-сетью:  

$$
y=r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta^{-})
$$  

Тогда правило, по которому строится целевая переменная, будет меняться раз в 1000 шагов, и мы 1000 шагов будем решать одну и ту же задачу регрессии. Такой процесс будет намного стабильнее.  

# Experience Replay  

Чтобы окончательно собрать алгоритм Deep Q-learning (обычно называемый DQN, Deep Qnetwork), нам понадобится сделать последний шаг, связанный опять со сбором данных. Коли мы хотим обучать нейросетку, нам нужно для каждого обновления весов откуда-то взять целый мини-батч данных, то есть батч переходов $\left(s,a,r,s^{\prime}\right)$ , чтобы по нему усреднить оценку градиента. Однако, если мы возьмём среду, сделаем в ней $N$ шагов, то встреченные нами $N$ переходов будут очень похожи друг на друга: они все придут из одной и той же области пространства состояний. Обучение нейросетки на скоррелированных данных — плохая идея, поскольку такая модель быстро забудет, что она учила на прошлых итерациях.  

Бороться с этой проблемой можно двумя способами. Первый способ, доступный всегда, когда среда задана при помощи виртуального симулятора — запуск параллельных агентов. Запускается параллельно $N$ процессов взаимодействия агента со средой, и для того, чтобы собрать очередной мини-батч переходов для обучения, во всех экземплярах проводится по одному шагу взаимодействия, собирается по одному переходику. Такой мини-батч уже будет разнообразным.  

Более интересный второй способ. Давайте после очередного шага взаимодействия со средой мы не будем тут же использовать переход $\left(s,a,r,s^{\prime}\right)$ для обновления модели, а запомним этот переход и положим его себе в коллекцию. Память со всеми встретившимися в ходе проб и ошибок переходами $\left(s,a,r,s^{\prime}\right)$ называется реплей буфером (replay buffer или experience replay). Теперь для того, чтобы обновить веса нашей сети, мы возьмём и случайно засэмплируем из равномерного распределения желаемое количество переходов из всей истории.  

Однако, использование реплей буфера возможно далеко не во всех алгоритмах обучения с подкреплением. Дело в том, что некоторые алгоритмы обучения с подкреплением требуют, чтобы данные для очередного шага обновления весов были сгенерированы именно текущей, самой свежей версией стратегии. Такие алгоритмы относят к классу on-policy: они могут улучшать стратегию только по данным из неё же самой («on policy»). Примером on-policy алгоритмов выступают, например, эволюционные алгоритмы. Как они устроены: например, можно завести популяцию стратегий, поиграть каждой со средой, отобрать лучшие и как-то породить новую популяцию (подробнее про одну из самых успешных схем в рамках такой идеи можно посмотреть здесь). Как бы ни была устроена эта схема, эволюционный алгоритм никак не может использовать данные из, например, старых, плохих стратегий, которые вели себя, скажем, не сильно лучше случайной стратегии. Поэтому неизбежно в эволюционном подходе нужно свежую популяцию отправлять в среду и собирать новые данные перед каждым следующим шагом.  

И вот важный момент: Deep Q-learning, как и обычный Q-learning, относится к off-policy алгоритмам обучения с подкреплением. Совершенно неважно, какая стратегия, умная или не очень, старая или новая, породила переход $\left(s,a,r,s^{\prime}\right)$ , нам всё равно нужно решать уравнение оптимальности Беллмана в том числе и для этой пары $s,a$ и нам достаточно при построении таргета лишь чтобы $s^{\prime}$ был сэмплом из функции переходов (а она-то как раз одна вне зависимости от того, какая стратегия взаимодействует в среде). Поэтому обновлять модель $Q^{*}(s,a)$ мы можем по совершенно произвольному опыту, и, значит, мы в том числе можем использовать experience replay.  

![](images/fdd4f4f98ab86534dbfa0e771a3f5c6da8a3708a2e04cf2bcff117b78b97f4a9.jpg)  
источник картинки — курс UC Berkeley AI  

В любом случае, даже в сложных средах, при взаимодействии со средой мы всё равно должны как-то разрешить дилемму exploration-exploitation, и пользоваться, например, $\varepsilon$ -жадной стратегией исследования. Итак, алгоритм DQN выглядит так:  

1. Проинициализировать нейросеть $Q^{*}(s,a,\theta)$ .  

2. Проинициализировать таргет-сеть, положив $\theta^{-}=\theta$ .  

3. Пронаблюдать $s_{0}$ ​ из среды.  

4. Для $k=0,1,2,$ … :  

с вероятностью $\varepsilon$ выбрать действие $a_{k}$ ​ случайно, иначе жадно:  

$$
a_{k}=\mathrm{argmax}_{a_{k}}Q^{*}(s_{k},a_{k},\theta)
$$  

отправить действие $a_{k}$ ​ в среду, получить награду за шаг $r_{k}$ ​ и следующее состояние $s_{k+1}$ ​.   
добавить переход $(s_{k},a_{k},r_{k},s_{k+1})$ в реплей буфер.   
если в реплей буфере скопилось достаточное число переходиков, провести шаг обучения. Для этого сэмплируем мини-батч переходиков $\left(s,a,r,s^{\prime}\right)$ из буфера.   
для каждого переходика считаем целевую переменную: $y=r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta^{-})$   
сделать шаг градиентного спуска для обновления θ, минимизируя  

$$
{\sum}(y-~Q^{*}(s,a,\theta~))^{2}
$$  

если $k$ делится на 1000, обновить таргет-сеть: $\theta^{-}\leftarrow\theta$ .  

Алгоритм DQN не требует никаких handcrafted признаков или специфических настроек под заданную игру. Один и тот же алгоритм, с одними и теми же гиперпараметрами, можно запустить на любой из 57 игр древней консоли Atari (пример игры в Breakout) и получить какую- то стратегию. Для сравнения алгоритмов RL между собой результаты обычно усредняют по всем 57 играм Atari. Недавно алгоритм под названием Agent57, объединяющий довольно много модификаций и улучшений DQN и развивающий эту идею, смог победить человека сразу во всех этих 57 играх.  

# А если пространство действий непрерывно?  

Всюду в DQN мы предполагали, что пространство действий дискретно и маленькое, чтобы мы могли считать жадную стратегию $\pi(s)=\mathrm{argmax}_{a}Q^{*}(s,a,\theta) $ и считать максимум в формуле целевой переменной $\operatorname*{max}_{a}Q^{*}(s,a,\theta)$ . Если пространство действий непрерывно, и на каждом шаге от агента ожидается выбор нескольких вещественных чисел, то как это делать непонятно. Такая ситуация повсюду возникает в робототехнике. Там каждое сочленение робота можно, например, поворачивать вправо / влево, и такие действия проще описывать набором чисел в диапазоне [-1, 1], где -1 — крайне левое положение, $+1$ — крайне правое, и доступны любые промежуточные варианты. При этом дискретизация действий не вариант из-за экспоненциального взрыва числа вариантов и потери семантики действий. Нам, в общем-то, нужно в DQN только одну проблему решить: как-то научиться аргмаксимум по действиям брать  

А давайте, коли мы не знаем argmaxa​ $Q^{*}(s,a)$ , приблизим его другой нейросеткой. А то есть, заведём вторую нейросеть $\pi(s,\phi)$ с параметрами $\phi$ , и будем учить её так, чтобы  

$$
\begin{array}{r}{\pi(s,\phi)\approx\mathrm{~argmax}_{a}Q^{*}(s,a,\theta).}\end{array}
$$  

Как это сделать? Ну, будем на каждой итерации алгоритма брать батч состояний s из нашего реплей буфера и будем учить $\pi(s,\phi)$ выдавать такие действия, на которых наша Q-функция выдаёт большие скалярные значения:  

$$
\sum_{s}Q^{*}(s,\pi(s,\phi),\theta){\to}\quad\operatorname*{max}_{\phi}
$$  

Причём, поскольку действия непрерывные, всё слева дифференцируемо и мы можем напрямую применять самый обычный backpropagation!  

![](images/5fec680b4e840851ae0268d87cba6c6ddeb4887b5264794c7eb7a8f562e71abc.jpg)  

Теперь когда на руках есть приближение $\begin{array}{r l}{\pi(s,\phi)\approx}&{{}\operatorname*{argmax}_{a}Q^{*}(s,a,\theta)}\end{array}$ , можно просто использовать его всюду, где нам нужны аргмаксимумы и максимумы от нашей Q-функции. Мы получили Actor-Critic схему: у нас есть актёр, $\pi(s,\phi)$ — детерминированная стратегия, и критик $Q^{*}(s,a)$ , который оценивает выбор действий актёром и предоставляет градиент для его улучшения. Актёр учится выбирать действия, которые больше всего нравятся критику, а критик учится регрессией с целевой переменной  

$$
y=r+\gamma\operatorname*{max}_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta^{-})\approx r+\gamma Q^{*}(s^{\prime},\pi\left(s^{\prime},\phi\right),\theta^{-})
$$  

Эта прикольная рабочая эвристика позволяет придумать off-policy алгоритмы для непрерывных пространств действий; к такому подходу относятся такие алгоритмы, как DDPG, TD3 и SAC.  

# Policy Gradient алгоритмы  

В рассмотренных алгоритмах есть несколько приниципиальных ограничений, которые вытекают непосредственно из самой идеи подхода. Мы учимся с таргетов, заглядывающих всего на один шаг вперёд, использующих только s ; это чревато проблемой накапливающейся ошибки, поскольку если между выполнением действия и получением награды $+1$ проходит 100 шагов, нам нужно на сто шагов «распространять» полученный сигнал. Мы должны учить  

$Q^{*}(s,a)$ вместо того, чтобы как-то напрямую («end-to-end») запомнить, какие действия в каких состояниях хорошие. Наконец, наша стратегия всегда детерминирована, когда для взаимодействия со средой во время сбора данных, например, нам позарез нужна была стохастичная, чтобы гарантированно обновлять Q-функцию для всех пар s,a , и эту проблему пришлось закрывать костылями.  

Есть второй подход model-free алгоритмов RL, называемый Policy Gradient, который позволяет избежать вышеперечисленных недостатков за счёт on-policy режима работы. Идея выглядит так: давайте будем искать стратегию в классе стохастичных стратегий, то есть заведём нейросеть, моделирующую $\pi_{\boldsymbol{\theta}}(\boldsymbol{a}|\mathrm{\Delta}s)$ напрямую. Тогда наш функционал, который мы оптимизируем,  

$$
J(\theta)=\mathbb{E}_{T\sim\pi_{\theta}}\sum_{t\geq0}\gamma^{t}r_{t}\to\operatorname*{max}_{\theta},
$$  

дифференцируем по параметрам $\theta$ , и градиент равен:  

$$
\nabla_{\theta}J(\theta)=\mathbb{E}_{T\sim\pi_{\theta}}\sum_{t\geq0}\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t})\gamma^{t}R_{t},
$$  

где $R_{t}$ - reward-to-go с шага $t$ , то есть награда, собранная в сыгранном эпизоде после шага $t$ :  

$$
R_{t}=\sum_{t^{\prime}\geq t}\gamma^{t^{\prime}-t}r_{t^{\prime}}
$$  

# Скетч доказательства  

Эта формула говорит нам, что градиент нашего функционала — это тоже мат.ожидание по траекториям. А значит, мы можем попробовать посчитать какую-то оценку этого градиента, заменив мат.ожидание на Монте Карло оценку, и просто начать оптимизировать наш функционал самым обычным стохастическим градиентным спуском! А то есть: берём нашу стратегию $\pi_{\theta}$ ​ с текущими значениями параметров θ, играем эпизод (или несколько) в среде, то есть сэмплируем $\tau\sim$ $\pi_{\theta}$ ​, и затем делаем шаг градиентного подъёма:  

$$
\theta\gets\theta+\alpha\sum_{t\geq0}\nabla_{\theta}\log\pi_{\theta}(a_{t}\mid s_{t})\gamma^{t}R_{t}
$$  

Почему эта идея приводит к on-policy подходу? Для каждого шага градиентного шага нам обязательно нужно взять $\tau\sim$ πθ​ с самыми свежими, с текущими весами $\theta$ , и никакая другая траектория, порождённая какой-то другой стратегией, нам не подойдёт. Поэтому для каждой итерации алгоритма нам придётся заново играть очередной эпизод со средой. Это sampleinefficient: неэффективно по числу сэмплов, мы собираем слишком много данных и очень неэффективно с ними работаем.  

Policy Gradient алгоритмы пытаются по-разному бороться с этой неэффективностью, опять же обращаясь к теории оценочных функций и бутстрапированным оценкам, позволяющим предсказывать будущие награды, не доигрывая эпизоды целиком до конца. Большинство этих алгоритмов остаются в on-policy режиме и применимы в любых пространствах действий. К этим алгоритмам относятся такие алгоритмы, как Advantage Actor-Critic (A2C), Trust-Region Policy Optimization (TRPO) и Proximal Policy Optimization (PPO).  

# Что там ещё?  

Мы до сих пор разбирали model-free алгоритмы RL, которые обходились без знаний о $p(s^{\prime})$ s,a ) и никак не пытались приближать это распределение. Однако, в каких-нибудь пятнашках функция переходов нам известна: мы знаем, в какое состояние перейдёт среда, если мы выберем некоторое действие в таком-то состоянии. Понятно, что эту информацию было бы здорово как-то использовать. Существует обширный класс model-based, который либо предполагает, что функция переходов дана, либо мы учим её приближение, используя $s,a$ , $s^{\prime}$ из нашего опыта в качестве обучающей выборки. Алгоритм AlphaZero на основе этого подхода превзошёл человека в игру Го, которая считалась куда более сложной игрой, чем шахматы; причём этот алгоритм возможно запустить обучаться на любой игре: как на Го, так и на шахматах или сёги.  

![](images/2f04bdb68419d5ed92bd9370f2e7e5dbf0999e0a6cdd07688cc01ad5e750d32d.jpg)  

# источник картинки — курс UC Berkeley AI  

Обучение с подкреплением стремится построить алгоритмы, способные обучаться решать любую задачу, представленную в формализме MDP. Как и обычные методы оптимизации, их можно использовать в виде чёрной коробочки из готовых библиотек, например, OpenAI Stable  

Baselines. Внутри таких коробочек будет, однако, довольно много гиперпараметров, которые пока не совсем понятно как настраивать под ту или иную практическую задачу. И хотя успехи Deep RL демонстрируют, что эти алгоритмы способны обучаться невероятно сложным задачам вроде победы над людьми в Dota 2 и в StarCraft II, они требуют для этого колоссального количества ресурсов. Поиск более эффективных процедур — открытая задача в Deep RL.  

В ШАДе есть курс Practical RL, на котором вы погрузитесь глубже в мир глубокого обучения с подкреплением, разберётесь в более продвинутых алгоритмах и попробуете пообучать нейронки решать разные задачки в разных средах.  

# Параграф не прочитан  

Отмечайте параграфы как прочитанные чтобы видеть свой прогресс обучения  

Вступайте в сообщество хендбука   
Здесь можно найти единомышленников, экспертов и просто интересных собеседников. А ещё — получить помощь или поделиться знаниями.  

Вступить  

$\bigcirc$ Сообщить об ошибке  

Предыдущий параграф  

10.5. Задача ранжирования  

Следующий параграф  

11.2. Краудсорсинг  

# AHneKc O6pa30BaHNe  

Исследования   
Хендбуки   
База знаний   
Журнал   
События   
Яндекс Учебник   
Яндекс Лицей   
Яндекс Практикум   
Школа анализа данных   
Программы в университетах   
О нас   
Обратная связь   
Партнерам   
Сведения   
об образовательной   
организации   
Пользовательское   
соглашение хендбуков  

Рассылка Бот K  