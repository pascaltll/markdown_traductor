{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4788fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5c484d05ec4e3bb7bab5f55c996eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefc25075d9444c3bd9c925573aa3e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4f9bd90f484512aa631c26cc3dcdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c7bfdf36984f8bac40897c3ccb879d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m correction_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhind/Phind-CodeLlama-34B-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m correction_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(correction_model_name)\n\u001b[0;32m---> 19\u001b[0m correction_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrection_model_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Patrones\u001b[39;00m\n\u001b[1;32m     23\u001b[0m image_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m((.*?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:487\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    485\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Configuración del dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_name = \"facebook/m2m100_418M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang=\"ru\", tgt_lang=\"en\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Cargar modelo y tokenizer para corrección tipográfica (usando BERT como ejemplo)\n",
    "correction_model_name = \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "correction_tokenizer = AutoTokenizer.from_pretrained(correction_model_name)\n",
    "correction_model = AutoModelForSeq2SeqLM.from_pretrained(correction_model_name).to(device)\n",
    "\n",
    "\n",
    "# Patrones\n",
    "image_pattern = r\"!\\[\\]\\((.*?)\\)\"\n",
    "latex_inline_pattern = r\"\\$[^$]+\\$\"\n",
    "latex_block_pattern = r\"\\$\\$[\\s\\S]*?\\$\\$\"\n",
    "code_block_pattern = r\"```[\\s\\S]*?```\"\n",
    "header_pattern = r\"^(#{1,6})\\s+(.*)$\"\n",
    "\n",
    "# Diccionarios para preservar bloques LaTeX\n",
    "latex_blocks = {}\n",
    "block_counter = 0\n",
    "\n",
    "\n",
    "def translate_text(text):\n",
    "    tokenizer.src_lang = \"ru\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "    translated = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"en\"), max_length=1024)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def preserve_latex_blocks(text):\n",
    "    global block_counter\n",
    "    def repl(match):\n",
    "        global block_counter\n",
    "        key = f\"<LATEX_BLOCK_{block_counter}>\"\n",
    "        latex_blocks[key] = match.group(0)\n",
    "        block_counter += 1\n",
    "        return key\n",
    "    return re.sub(latex_block_pattern, repl, text)\n",
    "\n",
    "\n",
    "def restore_latex_blocks(text):\n",
    "    for key, value in latex_blocks.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_paragraph(paragraph):\n",
    "    patterns = [image_pattern, latex_block_pattern, latex_inline_pattern, code_block_pattern]\n",
    "    parts = [paragraph]\n",
    "    for pattern in patterns:\n",
    "        new_parts = []\n",
    "        for part in parts:\n",
    "            matches = list(re.finditer(pattern, part, re.DOTALL))\n",
    "            last_pos = 0\n",
    "            for match in matches:\n",
    "                start, end = match.span()\n",
    "                if last_pos < start:\n",
    "                    new_parts.append(part[last_pos:start])\n",
    "                new_parts.append(match.group(0))\n",
    "                last_pos = end\n",
    "            if last_pos < len(part):\n",
    "                new_parts.append(part[last_pos:])\n",
    "        parts = new_parts\n",
    "    return parts\n",
    "\n",
    "\n",
    "def is_russian(text):\n",
    "    return re.search(r\"[а-яА-Я]\", text, re.DOTALL)\n",
    "\n",
    "def correct_latex_in_text(text):\n",
    "    latex_pattern = re.compile(r\"(\\${1,2})([^\\$]+?)\\1\", re.DOTALL)\n",
    "    matches = list(latex_pattern.finditer(text))\n",
    "\n",
    "    corrected_latex = {}\n",
    "    for match in tqdm(matches, desc=\"Corrigiendo LaTeX\"):\n",
    "        delimiter, formula = match.groups()\n",
    "        prompt = f\"Corrige cualquier error tipográfico o sintáctico en esta fórmula LaTeX sin cambiar su contenido matemático:\\n\\n{delimiter}{formula}{delimiter}\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "        corrected = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Quitar el prompt si el modelo responde con texto de más\n",
    "        corrected_formula = re.search(r\"(\\${1,2})([^\\$]+?)\\1\", corrected)\n",
    "        if corrected_formula:\n",
    "            corrected = f\"{delimiter}{corrected_formula.group(2)}{delimiter}\"\n",
    "        else:\n",
    "            corrected = f\"{delimiter}{formula.strip()}{delimiter}\"\n",
    "\n",
    "        corrected_latex[match.group(0)] = corrected\n",
    "\n",
    "    # Reemplazar en el contenido original\n",
    "    for original, fixed in corrected_latex.items():\n",
    "        text = text.replace(original, fixed)\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_paragraph(paragraph):\n",
    "    header_match = re.match(header_pattern, paragraph)\n",
    "    if header_match:\n",
    "        level, text = header_match.groups()\n",
    "        if is_russian(text):\n",
    "            text = translate_text(text)\n",
    "        return f\"{level} {text}\"\n",
    "\n",
    "    paragraph = preserve_latex_blocks(paragraph)\n",
    "    parts = split_paragraph(paragraph)\n",
    "\n",
    "    translated_parts = []\n",
    "    for part in parts:\n",
    "        if is_russian(part):\n",
    "            translated_parts.append(translate_text(part))\n",
    "        elif re.match(latex_inline_pattern, part) or re.match(latex_block_pattern, part):\n",
    "            # Si es texto LaTeX, corregirlo\n",
    "            translated_parts.append(correct_latex_in_text(part))\n",
    "        else:\n",
    "            translated_parts.append(part)\n",
    "\n",
    "    final_paragraph = \"\".join(translated_parts)\n",
    "    return restore_latex_blocks(final_paragraph).strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = \"full.md\"\n",
    "    output_file = \"output.md\"\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    paragraphs = content.split(\"\\n\\n\")\n",
    "    translated_paragraphs = []\n",
    "\n",
    "    for paragraph in tqdm(paragraphs, desc=\"Translating markdown paragraphs\"):\n",
    "        translated_paragraph = process_paragraph(paragraph)\n",
    "        translated_paragraphs.append(translated_paragraph)\n",
    "\n",
    "    translated_content = \"\\n\\n\".join(translated_paragraphs)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_content)\n",
    "\n",
    "    print(\"Translation complete! Check 'output.md' for the result.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148de134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d100074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
